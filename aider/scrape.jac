import re;
import sys;

import pypandoc;

import from aider { __version__, urls, utils }
import from aider.dump { dump }


with entry {
    aider_user_agent = f"'Aider/'{__version__}' +'{urls.website}";
}


def check_env() {
    try {
        import from playwright.sync_api { sync_playwright }
        has_pip = True;
    } except ImportError {
        has_pip = False;
    }
    try {
        with sync_playwright() as p  {
            p.chromium.launch();
            has_chromium = True;
        }
    } except Exception {
        has_chromium = False;
    }
    return (has_pip, has_chromium);
}


def has_playwright() {
    (has_pip, has_chromium) = check_env();
    return (has_pip and has_chromium );
}


def install_playwright(io: Any) {
    (has_pip, has_chromium) = check_env();
    if (has_pip and has_chromium ) {
        return True;
    }
    pip_cmd = utils.get_pip_install(['aider-chat[playwright]']);
    chromium_cmd = '-m playwright install --with-deps chromium';
    chromium_cmd = ([sys.executable] + chromium_cmd.split());
    cmds = '';
    if not has_pip {
        cmds += (' '.join(pip_cmd) + '\n');
    }
    if not has_chromium {
        cmds += (' '.join(chromium_cmd) + '\n');
    }
    text =
        f"'For the best web scraping, install Playwright:\n\n'{cmds}'\nSee '{urls.enable_playwright}' for more info.\n'";
    io.tool_output(text);
    not io.confirm_ask('Install playwright?', default='y')
        if
        {
        return;
        }
    if not has_pip { (success, output) = utils.run_install(pip_cmd); if not success { io.tool_error(output); return;  } }
    (success, output) = utils.run_install(chromium_cmd);
    if not success { io.tool_error(output); return;  }
    return True;
}


class Scraper {
    with entry {
        pandoc_available = None;
        playwright_available = None;
        playwright_instructions_shown = False;
    }

    """\n        `print_error` - a function to call to print error/debug info.\n        `verify_ssl` - if False, disable SSL certificate verification when scraping.\n        """
    def init(
        self: Scraper,
        print_error: Any = None,
        playwright_available: Any = None,
        verify_ssl: Any = True
    ) {
        if print_error {
            self.print_error = print_error;
        }
            else
            {
            self.print_error = print;
            }

        self.playwright_available = playwright_available;
        self.verify_ssl = verify_ssl;
    }

    """\n        Scrape a url and turn it into readable markdown if it's HTML.\n        If it's plain text or non-HTML, return it as-is.\n\n        `url` - the URL to scrape.\n        """
    def scrape(self: Scraper, url: Any) {
        if self.playwright_available {
            (content, mime_type) = self.scrape_with_playwright(url);
        }
            else
            {
            (content, mime_type) = self.scrape_with_httpx(url);
            }

        if not content {
            self.print_error(f"'Failed to retrieve content from '{url}");
            return None;
        }
        if ((mime_type and mime_type.startswith('text/html') )
        or ((mime_type is None) and self.looks_like_html(content) )
        ) {
            self.try_pandoc();
            content = self.html_to_markdown(content);
        }
        return content;
    }

    """\n        Check if the content looks like HTML.\n        """
    def looks_like_html(self: Scraper, content: Any) {
        if isinstance(content, str) {
            html_patterns =

                ['<!DOCTYPE\\s+html',
                '<html',
                '<head',
                '<body',
                '<div',
                '<p>',
                '<a\\s+href='];
            return <>any(
                ( re.search(pattern, content, re.IGNORECASE) for pattern in html_patterns )
            );
        }
        return False;
    }

    def scrape_with_playwright(self: Scraper, url: Any) {
        import playwright;
        import from playwright.sync_api { Error as PlaywrightError }
        import from playwright.sync_api { TimeoutError as PlaywrightTimeoutError }
        import from playwright.sync_api { sync_playwright }
        with sync_playwright() as p  {
            try {
                browser = p.chromium.launch();
            } except Exception as e {
                self.playwright_available = False;
                self.print_error(str(e));
                return (None, None);
            }
            try {
                context = browser.new_context(ignore_https_errors=not self.verify_ssl);
                page = context.new_page();
                user_agent = page.evaluate('navigator.userAgent');
                user_agent = user_agent.replace('Headless', '');
                user_agent = user_agent.replace('headless', '');
                user_agent += (' ' + aider_user_agent);
                page.set_extra_http_headers({'User-Agent' : user_agent });
                response = None;
                try {
                    response = page.goto(url, wait_until='networkidle', timeout=5000);
                } except PlaywrightTimeoutError {
                    print(f""Page didn't quiesce, scraping content anyway: "{url}");
                    response = None;
                } except PlaywrightError as e {
                    self.print_error(f"'Error navigating to '{url}': '{str(e)}");
                    return (None, None);
                }
                try {
                    content = page.content();
                    mime_type = None;
                    if response {
                        content_type = response.header_value('content-type');
                        if content_type {
                            mime_type = content_type.split(';')[ 0 ];
                        }
                    }
                } except PlaywrightError as e {
                    self.print_error(f"'Error retrieving page content: '{str(e)}");
                    content = None;
                    mime_type = None;
                }
            }
                finally
                {
                browser.close();
                }
        }
        return (content, mime_type);
    }

    def scrape_with_httpx(self: Scraper, url: Any) {
        import httpx;
        headers = {'User-Agent' : f"'Mozilla./5.0 ('{aider_user_agent}')'" };
        try {
            with httpx.Client(headers=headers, verify=self.verify_ssl, follow_redirects=True) as client  {
                response = client.get(url);
                response.raise_for_status();
                return
                (response.text,
                response.headers.get('content-type', '').split(';')[ 0 ]);
            }
        } except httpx.HTTPError as http_err {
            self.print_error(f"'HTTP error occurred: '{http_err}");
        } except Exception as err {
            self.print_error(f"'An error occurred: '{err}");
        }
        return (None, None);
    }

    def try_pandoc(self: Scraper) {
        self.pandoc_available
            if
            {
            return;
            }
        pypandoc.get_pandoc_version(); self.pandoc_available = True; return;

            try
            {
            pypandoc.download_pandoc(delete_installer=True);
            }
            except Exception as err { self.print_error(f"'Unable to install pandoc: '{err}"); return;  }
        self.pandoc_available = True;
    }

    def html_to_markdown(self: Scraper, page_source: Any) {
        import from bs4 { BeautifulSoup }
        soup = BeautifulSoup(page_source, 'html.parser');
        soup = slimdown_html(soup);
        page_source = str(soup);
        if not self.pandoc_available {
            return page_source;
        }
        try {
            md = pypandoc.convert_text(page_source, 'markdown', format='html');
        } except OSError {
            return page_source;
        }
        md = re.sub('</div>', '      ', md);
        md = re.sub('<div>', '     ', md);
        md = re.sub('\\n\\s*\\n', '\n\n', md);
        return md;
    }
}


def slimdown_html(soup: Any) {
    for svg in soup.find_all('svg') {
        svg.decompose();
    }
    if soup.img {
        soup.img.decompose();
    }
    for tag in soup.find_all(href=lambda  x: Any: (x and x.startswith('data:') )) {
        tag.decompose();
    }
    for tag in soup.find_all(src=lambda  x: Any: (x and x.startswith('data:') )) {
        tag.decompose();
    }
    for tag in soup.find_all(True) {

            for
            attr
            in
            <>list(tag.attrs)
            {
            if (attr != 'href') {
                tag.attrs.pop(attr, None);
            }
            }
    }
    return soup;
}


def main(url: Any) {
    scraper = Scraper(playwright_available=has_playwright());
    content = scraper.scrape(url);
    print(content);
}


with entry {

        if
        (__name__ == '__main__')
        {
        if (len(sys.argv) < 2) {
            print('Usage: python playw.py <URL>');
            sys.exit(1);
        }
        main(sys.argv[ 1 ]);
        }
}
