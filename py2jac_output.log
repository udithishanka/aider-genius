import difflib;
import hashlib;
import importlib.resources;
import json;
import math;
import os;
import platform;
import sys;
import time;
import from dataclasses { dataclass, fields }
import from datetime { datetime }
import from pathlib { Path }
import from typing { Optional, Union }

import json5;
import yaml;
import from PIL { Image }

import from aider { __version__ }
import from aider.dump { dump }
import from aider.llm { litellm }
import from aider.openrouter { OpenRouterModelManager }
import from aider.sendchat { ensure_alternating_roles, sanity_check_messages }
import from aider.utils { check_pip_install_extra }


with entry {
    RETRY_TIMEOUT = 60;
    request_timeout = 600;
    DEFAULT_MODEL_NAME = 'gpt-4o';
    ANTHROPIC_BETA_HEADER = 'prompt-caching-2024-07-31,pdfs-2024-09-25';
    OPENAI_MODELS =
        '\no1\no1-preview\no1-mini\no3-mini\ngpt-4\ngpt-4o\ngpt-4o-2024-05-13\ngpt-4-turbo-preview\ngpt-4-0314\ngpt-4-0613\ngpt-4-32k\ngpt-4-32k-0314\ngpt-4-32k-0613\ngpt-4-turbo\ngpt-4-turbo-2024-04-09\ngpt-4-1106-preview\ngpt-4-0125-preview\ngpt-4-vision-preview\ngpt-4-1106-vision-preview\ngpt-4o-mini\ngpt-4o-mini-2024-07-18\ngpt-3.5-turbo\ngpt-3.5-turbo-0301\ngpt-3.5-turbo-0613\ngpt-3.5-turbo-1106\ngpt-3.5-turbo-0125\ngpt-3.5-turbo-16k\ngpt-3.5-turbo-16k-0613\n';
    OPENAI_MODELS = [ ln.strip() for ln in OPENAI_MODELS.splitlines() if ln.strip() ];
    ANTHROPIC_MODELS =
        '\nclaude-2\nclaude-2.1\nclaude-3-haiku-20240307\nclaude-3-5-haiku-20241022\nclaude-3-opus-20240229\nclaude-3-sonnet-20240229\nclaude-3-5-sonnet-20240620\nclaude-3-5-sonnet-20241022\nclaude-sonnet-4-20250514\nclaude-opus-4-20250514\n';
    ANTHROPIC_MODELS =
        [ ln.strip() for ln in ANTHROPIC_MODELS.splitlines() if ln.strip() ];
    MODEL_ALIASES =

        {'sonnet' : 'anthropic/claude-sonnet-4-20250514' , 'haiku' : 'claude-3-5-haiku-20241022' , 'opus' : 'claude-opus-4-20250514' , '4' : 'gpt-4-0613' , '4o' : 'gpt-4o' , '4-turbo' : 'gpt-4-1106-preview' , '35turbo' : 'gpt-3.5-turbo' , '35-turbo' : 'gpt-3.5-turbo' , '3' : 'gpt-3.5-turbo' , 'deepseek' : 'deepseek/deepseek-chat' , 'flash' : 'gemini/gemini-2.5-flash' , 'quasar' : 'openrouter/openrouter/quasar-alpha' , 'r1' : 'deepseek/deepseek-reasoner' , 'gemini-2.5-pro' : 'gemini/gemini-2.5-pro' , 'gemini' : 'gemini/gemini-2.5-pro' , 'gemini-exp' : 'gemini/gemini-2.5-pro-exp-03-25' , 'grok3' : 'xai/grok-3-beta' , 'optimus' : 'openrouter/openrouter/optimus-alpha' };
}


@ dataclass
class ModelSettings {
    with entry {
        name: str;
        edit_format: str = 'whole';
        weak_model_name: Optional[ str ] = None;
        use_repo_map: bool = False;
        send_undo_reply: bool = False;
        lazy: bool = False;
        overeager: bool = False;
        reminder: str = 'user';
        examples_as_sys_msg: bool = False;
        extra_params: Optional[ <>dict ] = None;
        cache_control: bool = False;
        caches_by_default: bool = False;
        use_system_prompt: bool = True;
        use_temperature: Union[ (bool, float) ] = True;
        streaming: bool = True;
        editor_model_name: Optional[ str ] = None;
        editor_edit_format: Optional[ str ] = None;
        reasoning_tag: Optional[ str ] = None;
        remove_reasoning: Optional[ str ] = None;
        system_prompt_prefix: Optional[ str ] = None;
        accepts_settings: Optional[ <>list ] = None;
    }
}


with entry {
    MODEL_SETTINGS = [];
    with importlib.resources.open_text('aider.resources', 'model-settings.yml') as f  {
        model_settings_list = yaml.safe_load(f);
        for model_settings_dict in model_settings_list {
            MODEL_SETTINGS.append(ModelSettings(_=model_settings_dict));
        }
    }
}


class ModelInfoManager {
    with entry {
        MODEL_INFO_URL =
            'https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json';
        CACHE_TTL = ((60 * 60) * 24);
    }

    def init(self: ModelInfoManager) {
        self.cache_dir = ((Path.home() / '.aider') / 'caches');
        self.cache_file = (self.cache_dir / 'model_prices_and_context_window.json');
        self.content = None;
        self.local_model_metadata = {};
        self.verify_ssl = True;
        self._cache_loaded = False;
        self.openrouter_manager = OpenRouterModelManager();
    }

    def set_verify_ssl(self: ModelInfoManager, verify_ssl: Any) {
        self.verify_ssl = verify_ssl;
        if hasattr(self, 'openrouter_manager') {
            self.openrouter_manager.set_verify_ssl(verify_ssl);
        }
    }

    def _load_cache(self: ModelInfoManager) {
        self._cache_loaded
            if
            {
            return;
            }

            try
            {
            self.cache_dir.mkdir(parents=True, exist_ok=True);
            if self.cache_file.exists() {
                cache_age = (time.time() - self.cache_file.stat().st_mtime);
            if (cache_age < self.CACHE_TTL) {
                try {
                    self.content = json.loads(self.cache_file.read_text());
                } except json.JSONDecodeError {
                    self.content = None;
                }
            } }
            }
            OSError
                except
                {
                ;
                }
        self._cache_loaded = True;
    }

    def _update_cache(self: ModelInfoManager) {
        import requests; response =
            requests.get(self.MODEL_INFO_URL, timeout=5, verify=self.verify_ssl); if (response.status_code == 200) { self.content = response.json(); }
    }

    def get_model_from_cached_json_db(self: ModelInfoManager, model: Any) {
        data = self.local_model_metadata.get(model);
        if data {
            return data;
        }
        self._load_cache();
        if not self.content {
            self._update_cache();
        }
        if not self.content {
            return <>dict();
        }
        info = self.content.get(model, <>dict());
        if info {
            return info;
        }
        pieces = model.split('/');
        if (len(pieces) == 2) {
            info = self.content.get(pieces[ 1 ]);
        if (info and (info.get('litellm_provider') == pieces[ 0 ]) ) {
            return info;
        } }
        return <>dict();
    }

    def get_model_info(self: ModelInfoManager, model: Any) {
        cached_info = self.get_model_from_cached_json_db(model);
        litellm_info = None;
        if (litellm._lazy_module or not cached_info ) {
            try {
                litellm_info = litellm.get_model_info(model);
            } ex
                except
                Exception
                as
                {
                if ('model_prices_and_context_window.json' not in str(ex)) {
                    print(str(ex));
                }
                }
        }
        if litellm_info {
            return litellm_info;
        }
        if (not cached_info and model.startswith('openrouter/') ) {
            openrouter_info = self.openrouter_manager.get_model_info(model);
            if openrouter_info {
                return openrouter_info;
            }
            openrouter_info = self.fetch_openrouter_model_info(model);
            if openrouter_info {
                return openrouter_info;
            }
        }
        return cached_info;
    }

    """\n        Fetch model info by scraping the openrouter model page.\n        Expected URL: https://openrouter.ai/<model_route>\n        Example: openrouter/qwen/qwen-2.5-72b-instruct:free\n        Returns a dict with keys: max_tokens, max_input_tokens, max_output_tokens,\n        input_cost_per_token, output_cost_per_token.\n        """
    def fetch_openrouter_model_info(self: ModelInfoManager, model: Any) {
        url_part = model[ len('openrouter/') : ];
        url = ('https://openrouter.ai/' + url_part);

            try
            {
            import requests;
            response = requests.get(url, timeout=5, verify=self.verify_ssl);
            html = response.text;
            import re;
            if re.search(
                f"'The model\\s*.*'{re.escape(url_part)}'.* is not available'",
                html,
                re.IGNORECASE
            ) { print(f""\x1b[91mError: Model '"{url_part}"' is not available\x1b[0m""); return {};  }
            text = re.sub('<[^>]+>', ' ', html);
            context_match = re.search('([\\d,]+)\\s*context', text);
            if context_match {
                context_str = context_match.group(1).replace(',', '');
                context_size = int(context_str);
            }
                else
                {
                context_size = None;
                }

            input_cost_match =
                re.search('\\$\\s*([\\d.]+)\\s*/M input tokens', text, re.IGNORECASE);
            output_cost_match =
                re.search('\\$\\s*([\\d.]+)\\s*/M output tokens', text, re.IGNORECASE);
            input_cost =
                (float(input_cost_match.group(1)) / 1000000)
                if input_cost_match
                else None;
            output_cost =
                (float(output_cost_match.group(1)) / 1000000)
                if output_cost_match
                else None;
            params =

                {'max_input_tokens' : context_size , 'max_tokens' : context_size , 'max_output_tokens' : context_size , 'input_cost_per_token' : input_cost , 'output_cost_per_token' : output_cost };
            return params;
            }
            except Exception as e { print('Error fetching openrouter info:', str(e)); return {};  }

            if
            (response.status_code != 200)
            {
            return {};
            }
            if
            ((context_size is None) or (input_cost is None) or (output_cost is None) )
            {
            return {};
            }
    }
}


with entry {
    model_info_manager = ModelInfoManager();
}


class Model(ModelSettings) {
    def init(
        self: Model,
        model: Any,
        weak_model: Any = None,
        editor_model: Any = None,
        editor_edit_format: Any = None,
        verbose: Any = False
    ) {
        model = MODEL_ALIASES.get(model, model);
        self.name = model;
        self.verbose = verbose;
        self.max_chat_history_tokens = 1024;
        self.weak_model = None;
        self.editor_model = None;
        self.extra_model_settings =
            next(
                ( ms for ms in MODEL_SETTINGS if (ms.name == 'aider/extra_params') ),
                None
            );
        self.info = self.get_model_info(model);
        res = self.validate_environment();
        self.missing_keys = res.get('missing_keys');
        self.keys_in_environment = res.get('keys_in_environment');
        max_input_tokens = (self.info.get('max_input_tokens') or 0 );
        self.max_chat_history_tokens = min(max((max_input_tokens / 16), 1024), 8192);
        self.configure_model_settings(model);
        if (weak_model is False) {
            self.weak_model_name = None;
        }
            else
            {
            self.get_weak_model(weak_model);
            }

        if (editor_model is False) {
            self.editor_model_name = None;
        }
            else
            {
            self.get_editor_model(editor_model, editor_edit_format);
            }

    }

    def get_model_info(self: Model, model: Any) {
        return model_info_manager.get_model_info(model);
    }

    """Helper to copy fields from a ModelSettings instance to self"""
    def _copy_fields(self: Model, source: Any) {
        for field in fields(ModelSettings) {
            val = getattr(source, field.name);
            setattr(self, field.name, val);
        }
        if ((self.reasoning_tag is None) and (self.remove_reasoning is not None) ) {
            self.reasoning_tag = self.remove_reasoning;
        }
    }

    def configure_model_settings(self: Model, model: Any) {
        exact_match = False;
        ms MODEL_SETTINGS
            for
            in
            {
            if (model == ms.name) { self._copy_fields(ms); exact_match = True; break; }
            }
        if (self.accepts_settings is None) {
            self.accepts_settings = [];
        }
        model = model.lower();
        if not exact_match {
            self.apply_generic_model_settings(model);
        }

            if
            (self.extra_model_settings
            and self.extra_model_settings.extra_params
            and (self.extra_model_settings.name == 'aider/extra_params')
            )
            {
            if not self.extra_params {
                self.extra_params = {};
            }

                for
                (key, value)
                in
                self.extra_model_settings.extra_params.items()
                {
                if (isinstance(value, <>dict)
                and isinstance(self.extra_params.get(key), <>dict)
                ) {
                    self.extra_params[ key ] =
                        {** self.extra_params[ key ] , ** value };
                }
                    else
                    {
                    self.extra_params[ key ] = value;
                    }

                }
            }

            if
            self.name.startswith('openrouter/')
            {
            if (self.accepts_settings is None) {
                self.accepts_settings = [];
            }
            if ('thinking_tokens' not in self.accepts_settings) {
                self.accepts_settings.append('thinking_tokens');
            }
            if ('reasoning_effort' not in self.accepts_settings) {
                self.accepts_settings.append('reasoning_effort');
            }
            }
    }

    def apply_generic_model_settings(self: Model, model: Any) {
        if ('/o3-mini' in model) { self.edit_format = 'diff'; self.use_repo_map = True; self.use_temperature =
            False; self.system_prompt_prefix = 'Formatting re-enabled. '; self.system_prompt_prefix =
            'Formatting re-enabled. '; if ('reasoning_effort' not in self.accepts_settings) {
            self.accepts_settings.append('reasoning_effort');
        } return;  }
        if ('gpt-4.1-mini' in model) { self.edit_format = 'diff'; self.use_repo_map =
            True; self.reminder = 'sys'; self.examples_as_sys_msg = False; return;  }
        if ('gpt-4.1' in model) { self.edit_format = 'diff'; self.use_repo_map = True; self.reminder =
            'sys'; self.examples_as_sys_msg = False; return;  }
        if ('/o1-mini' in model) { self.use_repo_map = True; self.use_temperature =
            False; self.use_system_prompt = False; return;  }
        if ('/o1-preview' in model) { self.edit_format = 'diff'; self.use_repo_map =
            True; self.use_temperature = False; self.use_system_prompt = False; return;  }
        if ('/o1' in model) { self.edit_format = 'diff'; self.use_repo_map = True; self.use_temperature =
            False; self.streaming = False; self.system_prompt_prefix =
            'Formatting re-enabled. '; if ('reasoning_effort' not in self.accepts_settings) {
            self.accepts_settings.append('reasoning_effort');
        } return;  }
        if (('deepseek' in model) and ('v3' in model) ) { self.edit_format = 'diff'; self.use_repo_map =
            True; self.reminder = 'sys'; self.examples_as_sys_msg = True; return;  }
        if (('deepseek' in model) and (('r1' in model) or ('reasoning' in model) ) ) { self.edit_format =
            'diff'; self.use_repo_map = True; self.examples_as_sys_msg = True; self.use_temperature =
            False; self.reasoning_tag = 'think'; return;  }
        if ((('llama3' in model) or ('llama-3' in model) ) and ('70b' in model) ) { self.edit_format =
            'diff'; self.use_repo_map = True; self.send_undo_reply = True; self.examples_as_sys_msg =
            True; return;  }
        if (('gpt-4-turbo' in model) or (('gpt-4-' in model) and ('-preview' in model) ) ) { self.edit_format =
            'udiff'; self.use_repo_map = True; self.send_undo_reply = True; return;  }
        if (('gpt-4' in model) or ('claude-3-opus' in model) ) { self.edit_format =
            'diff'; self.use_repo_map = True; self.send_undo_reply = True; return;  }
        if (('gpt-3.5' in model) or ('gpt-4' in model) ) { self.reminder = 'sys'; return;  }
        if ('3-7-sonnet' in model) { self.edit_format = 'diff'; self.use_repo_map =
            True; self.examples_as_sys_msg = True; self.reminder = 'user'; if ('thinking_tokens' not in self.accepts_settings) {
            self.accepts_settings.append('thinking_tokens');
        } return;  }
        if (('3.5-sonnet' in model) or ('3-5-sonnet' in model) ) { self.edit_format =
            'diff'; self.use_repo_map = True; self.examples_as_sys_msg = True; self.reminder =
            'user'; return;  }
        if (model.startswith('o1-') or ('/o1-' in model) ) { self.use_system_prompt =
            False; self.use_temperature = False; return;  }
        if (('qwen' in model)
        and ('coder' in model)
        and (('2.5' in model) or ('2-5' in model) )
        and ('32b' in model)
        ) { self.edit_format = 'diff'; self.editor_edit_format = 'editor-diff'; self.use_repo_map =
            True; return;  }
        if (('qwq' in model) and ('32b' in model) and ('preview' not in model) ) { self.edit_format =
            'diff'; self.editor_edit_format = 'editor-diff'; self.use_repo_map = True; self.reasoning_tag =
            'think'; self.examples_as_sys_msg = True; self.use_temperature = 0.6; self.extra_params =
            <>dict(top_p=0.95); return;  }
        if (('qwen3' in model) and ('235b' in model) ) { self.edit_format = 'diff'; self.use_repo_map =
            True; self.system_prompt_prefix = '/no_think'; self.use_temperature = 0.7; self.extra_params =
            {'top_p' : 0.8 , 'top_k' : 20 , 'min_p' : 0.0 }; return;  }
        if (self.edit_format == 'diff') { self.use_repo_map = True; return;  }
    }

    def __str__(self: Model) {
        return self.name;
    }

    def get_weak_model(self: Model, provided_weak_model_name: Any) {
        if provided_weak_model_name {
            self.weak_model_name = provided_weak_model_name;
        }
        if not self.weak_model_name { self.weak_model = self; return;  }
        if (self.weak_model_name == self.name) { self.weak_model = self; return;  }
        self.weak_model = Model(self.weak_model_name, weak_model=False);
        return self.weak_model;
    }

    def commit_message_models(self: Model) {
        return [self.weak_model, self];
    }

    def get_editor_model(
        self: Model,
        provided_editor_model_name: Any,
        editor_edit_format: Any
    ) {
        if provided_editor_model_name {
            self.editor_model_name = provided_editor_model_name;
        }
        if editor_edit_format {
            self.editor_edit_format = editor_edit_format;
        }
        if (not self.editor_model_name or (self.editor_model_name == self.name) ) {
            self.editor_model = self;
        }
            else
            {
            self.editor_model = Model(self.editor_model_name, editor_model=False);
            }

        if not self.editor_edit_format { self.editor_edit_format =
            self.editor_model.edit_format; if (self.editor_edit_format in ('diff', 'whole', 'diff-fenced')) {
            self.editor_edit_format = ('editor-' + self.editor_edit_format);
        } }
        return self.editor_model;
    }

    def tokenizer(self: Model, text: Any) {
        return litellm.encode(model=self.name, text=text);
    }

    def token_count(self: Model, messages: Any) {
        if (<>type(messages) is <>list) {
            try {
                return litellm.token_counter(model=self.name, messages=messages);
            } except Exception as err {
                print(f"'Unable to count tokens: '{err}");
                return 0;
            }
        }
        not self.tokenizer
            if
            {
            return;
            }
        if (<>type(messages) is str) {
            msgs = messages;
        }
            else
            {
            msgs = json.dumps(messages);
            }

        try {
            return len(self.tokenizer(msgs));
        } except Exception as err {
            print(f"'Unable to count tokens: '{err}");
            return 0;
        }
    }

    """\n        Calculate the token cost for an image assuming high detail.\n        The token cost is determined by the size of the image.\n        :param fname: The filename of the image.\n        :return: The token cost for the image.\n        """
    def token_count_for_image(self: Model, fname: Any) {
        (width, height) = self.get_image_size(fname);
        max_dimension = max(width, height);
        if (max_dimension > 2048) {
            scale_factor = (2048 / max_dimension);
            width = int((width * scale_factor));
            height = int((height * scale_factor));
        }
        min_dimension = min(width, height);
        scale_factor = (768 / min_dimension);
        width = int((width * scale_factor));
        height = int((height * scale_factor));
        tiles_width = math.ceil((width / 512));
        tiles_height = math.ceil((height / 512));
        num_tiles = (tiles_width * tiles_height);
        token_cost = ((num_tiles * 170) + 85);
        return token_cost;
    }

    """\n        Retrieve the size of an image.\n        :param fname: The filename of the image.\n        :return: A tuple (width, height) representing the image size in pixels.\n        """
    def get_image_size(self: Model, fname: Any) {
        with Image.open(fname) as img  {
            return img.size;
        }
    }

    """Fast path for common models. Avoids forcing litellm import."""
    def fast_validate_environment(self: Model) {
        model = self.name;
        pieces = model.split('/');
        if (len(pieces) > 1) {
            provider = pieces[ 0 ];
        }
            else
            {
            provider = None;
            }

        keymap =
            <>dict(
                openrouter='OPENROUTER_API_KEY',
                openai='OPENAI_API_KEY',
                deepseek='DEEPSEEK_API_KEY',
                gemini='GEMINI_API_KEY',
                anthropic='ANTHROPIC_API_KEY',
                groq='GROQ_API_KEY',
                fireworks_ai='FIREWORKS_API_KEY'
            );
        var = None;
        if (model in OPENAI_MODELS) {
            var = 'OPENAI_API_KEY';
        } elif (model in ANTHROPIC_MODELS) {
            var = 'ANTHROPIC_API_KEY';
        }
            else
            {
            var = keymap.get(provider);
            }

        if (var and os.environ.get(var) ) {
            return <>dict(keys_in_environment=[var], missing_keys=[]);
        }
    }

    def validate_environment(self: Model) {
        res = self.fast_validate_environment();
        if res {
            return res;
        }
        model = self.name;
        res = litellm.validate_environment(model);

            if
            (res[ 'missing_keys' ]
            and <>any(
                ( (key in ['AWS_ACCESS_KEY_ID', 'AWS_SECRET_ACCESS_KEY']) for key in res[ 'missing_keys' ] )
            )
            )
            {
            if (model.startswith('bedrock/') or model.startswith('us.anthropic.') ) {
                if os.environ.get('AWS_PROFILE') {
                    res[ 'missing_keys' ] =
                        [ k for k in res[ 'missing_keys' ] if (k not in ['AWS_ACCESS_KEY_ID', 'AWS_SECRET_ACCESS_KEY']) ];
                if not res[ 'missing_keys' ] {
                    res[ 'keys_in_environment' ] = True;
                } }
            }
            }
        if res[ 'keys_in_environment' ] {
            return res;
        }
        if res[ 'missing_keys' ] {
            return res;
        }
        provider = self.info.get('litellm_provider', '').lower();
        if (provider == 'cohere_chat') {
            return validate_variables(['COHERE_API_KEY']);
        }
        if (provider == 'gemini') {
            return validate_variables(['GEMINI_API_KEY']);
        }
        if (provider == 'groq') {
            return validate_variables(['GROQ_API_KEY']);
        }
        return res;
    }

    def get_repo_map_tokens(self: Model) {
        map_tokens = 1024;
        max_inp_tokens = self.info.get('max_input_tokens');
        if max_inp_tokens {
            map_tokens = (max_inp_tokens / 8);
            map_tokens = min(map_tokens, 4096);
            map_tokens = max(map_tokens, 1024);
        }
        return map_tokens;
    }

    """Set the reasoning effort parameter for models that support it"""
    def set_reasoning_effort(self: Model, effort: Any) {
        if (effort is not None) {

                if
                self.name.startswith('openrouter/')
                {
                if not self.extra_params {
                    self.extra_params = {};
                }
                if ('extra_body' not in self.extra_params) {
                    self.extra_params[ 'extra_body' ] = {};
                }
                self.extra_params[ 'extra_body' ][ 'reasoning' ] = {'effort' : effort };
                }
                elif not self.extra_params {
                    self.extra_params = {};
                }
        }
    }

    """\n        Parse a token value string into an integer.\n        Accepts formats: 8096, "8k", "10.5k", "0.5M", "10K", etc.\n\n        Args:\n            value: String or int token value\n\n        Returns:\n            Integer token value\n        """
    def parse_token_value(self: Model, value: Any) {
        if isinstance(value, int) {
            return value;
        }
        if not isinstance(value, str) {
            return int(value);
        }
        value = value.strip().upper();
        if value.endswith('K') {
            multiplier = 1024;
            value = value[ : -1 ];
        } elif value.endswith('M') {
            multiplier = (1024 * 1024);
            value = value[ : -1 ];
        }
            else
            {
            multiplier = 1;
            }

        return int((float(value) * multiplier));
    }

    """\n        Set the thinking token budget for models that support it.\n        Accepts formats: 8096, "8k", "10.5k", "0.5M", "10K", etc.\n        Pass "0" to disable thinking tokens.\n        """
    def set_thinking_tokens(self: Model, value: Any) {
        if (value is not None) {
            num_tokens = self.parse_token_value(value);
            self.use_temperature = False;
            if {
                self.name.startswith('openrouter/')
                if ('extra_body' not in self.extra_params) {
                    self.extra_params[ 'extra_body' ] = {};
                }
                if (num_tokens > 0) {
                    self.extra_params[ 'extra_body' ][ 'reasoning' ] =
                        {'max_tokens' : num_tokens };
                } elif ('reasoning' in self.extra_params[ 'extra_body' ]) {
                    del (self.extra_params[ 'extra_body' ][ 'reasoning' ], ) ;
                }
            } elif (num_tokens > 0) {
                self.extra_params[ 'thinking' ] =
                    {'type' : 'enabled' , 'budget_tokens' : num_tokens };
            } elif ('thinking' in self.extra_params) {
                del (self.extra_params[ 'thinking' ], ) ;
            }
        if not self.extra_params {
            self.extra_params = {};
        } }
    }

    """Get formatted thinking token budget if available"""
    def get_raw_thinking_tokens(self: Model) {
        budget = None;
        if self.extra_params {
            if {
                self.name.startswith('openrouter/')
                if (('extra_body' in self.extra_params)
                and ('reasoning' in self.extra_params[ 'extra_body' ])
                and ('max_tokens' in self.extra_params[ 'extra_body' ][ 'reasoning' ])
                ) {
                    budget =
                        self.extra_params[ 'extra_body' ][ 'reasoning' ][ 'max_tokens' ];
                }
            } elif (('thinking' in self.extra_params)
            and ('budget_tokens' in self.extra_params[ 'thinking' ])
            ) {
                budget = self.extra_params[ 'thinking' ][ 'budget_tokens' ];
            }
        }
        return budget;
    }

    def get_thinking_tokens(self: Model) {
        budget = self.get_raw_thinking_tokens();

            if
            (budget is not None)
            {
            if (budget >= (1024 * 1024)) {
                value = (budget / (1024 * 1024));
            if (value == int(value)) {
                return f"{int(value)}'M'";
            }
                else
                {
                return f"{value}'M'";
                }
             }
                else
                {
                value = (budget / 1024);
                }
            if (value == int(value)) {
                return f"{int(value)}'k'";
            }
                else
                {
                return f"{value}'k'";
                }

            }
        return None;
    }

    """Get reasoning effort value if available"""
    def get_reasoning_effort(self: Model) {
        if self.extra_params {
            if {
                self.name.startswith('openrouter/')
                if (('extra_body' in self.extra_params)
                and ('reasoning' in self.extra_params[ 'extra_body' ])
                and ('effort' in self.extra_params[ 'extra_body' ][ 'reasoning' ])
                ) {
                    return self.extra_params[ 'extra_body' ][ 'reasoning' ][ 'effort' ];
                }
            } elif (('extra_body' in self.extra_params)
            and ('reasoning_effort' in self.extra_params[ 'extra_body' ])
            ) {
                return self.extra_params[ 'extra_body' ][ 'reasoning_effort' ];
            }
        }
        return None;
    }

    def is_deepseek_r1(self: Model) {
        name = self.name.lower();

            if
            ('deepseek' not in name)
            {
            return;
            }
        return (('r1' in name) or ('reasoner' in name) );
    }

    def is_ollama(self: Model) {
        return (self.name.startswith('ollama/') or self.name.startswith('ollama_chat/') );
    }

    def github_copilot_token_to_open_ai_key(self: Model, extra_headers: Any) {
        openai_api_key = 'OPENAI_API_KEY';
        if ((openai_api_key not in os.environ)
        or (int(
            <>dict(( x.split('=') for x in os.environ[ openai_api_key ].split(';') ))[ 'exp' ]
        ) < int(datetime.now().timestamp()))
        ) {
            import requests;
            """Custom exception for GitHub Copilot token-related errors."""
            class GitHubCopilotTokenError(Exception);
            github_token = os.environ[ 'GITHUB_COPILOT_TOKEN' ];
            headers =

                {'Authorization' : f"'Bearer '{os.environ[ 'GITHUB_COPILOT_TOKEN' ]}" , 'Editor-Version' : extra_headers[ 'Editor-Version' ] , 'Copilot-Integration-Id' : extra_headers[ 'Copilot-Integration-Id' ] , 'Content-Type' : 'application/json' };
            url = 'https://api.github.com/copilot_internal/v2/token';
            res = requests.get(url, headers=headers);
            response_data = res.json();
            token = response_data.get('token');
            os.environ[ openai_api_key ] = token;

            if
            ('GITHUB_COPILOT_TOKEN' not in os.environ)
            {
            raise KeyError('GITHUB_COPILOT_TOKEN environment variable not found') ;
            }
            if
            not github_token.strip()
            {
            raise KeyError('GITHUB_COPILOT_TOKEN environment variable is empty') ;
            } if (res.status_code != 200) {
            safe_headers =
                { k : v for (k, v) in headers.items() if (k != 'Authorization') };
            token_preview =
                (github_token[ : 5 ] + '...')
                if (len(github_token) >= 5)
                else github_token;
            safe_headers[ 'Authorization' ] = f"'Bearer '{token_preview}";
        raise GitHubCopilotTokenError(
            f"'GitHub Copilot API request failed (Status: '{res.status_code}')\nURL: '{url}'\nHeaders: '{json.dumps(safe_headers, indent=2)}'\nJSON: '{res.text}"
        ) ; }
            if
            not token
            {
            raise GitHubCopilotTokenError("Response missing 'token' field") ;
            } }
    }

    def send_completion(
        self: Model,
        messages: Any,
        functions: Any,
        stream: Any,
        temperature: Any = None
    ) {
        if os.environ.get('AIDER_SANITY_CHECK_TURNS') {
            sanity_check_messages(messages);
        }
        if self.is_deepseek_r1() {
            messages = ensure_alternating_roles(messages);
        }
        kwargs = <>dict(model=self.name, stream=stream);

            if
            (self.use_temperature is not False)
            {
            if (temperature is None) {
                if isinstance(self.use_temperature, bool) {
                    temperature = 0;
                }
                    else
                    {
                    temperature = float(self.use_temperature);
                    }

            }
            kwargs[ 'temperature' ] = temperature;
            }
        if (functions is not None) {
            function = functions[ 0 ];
            kwargs[ 'tools' ] = [<>dict(type='function', function=function)];
            kwargs[ 'tool_choice' ] =
                {'type' : 'function' , 'function' : {'name' : function[ 'name' ] } };
        }
        if self.extra_params {
            kwargs.update(self.extra_params);
        }
        if (self.is_ollama() and ('num_ctx' not in kwargs) ) {
            num_ctx = (int((self.token_count(messages) * 1.25)) + 8192);
            kwargs[ 'num_ctx' ] = num_ctx;
        }
        key = json.dumps(kwargs, sort_keys=True).encode();
        hash_object = hashlib.sha1(key);
        if ('timeout' not in kwargs) {
            kwargs[ 'timeout' ] = request_timeout;
        }
        if self.verbose {
            dump(kwargs);
        }
        kwargs[ 'messages' ] = messages;

            if
            ('GITHUB_COPILOT_TOKEN' in os.environ)
            {
            if ('extra_headers' not in kwargs) {
                kwargs[ 'extra_headers' ] =

                    {'Editor-Version' : f"'aider/'{__version__}" , 'Copilot-Integration-Id' : 'vscode-chat' };
            }
            self.github_copilot_token_to_open_ai_key(kwargs[ 'extra_headers' ]);
            }
        res = litellm.completion(_=kwargs);
        return (hash_object, res);
    }

    def simple_send_with_retries(self: Model, messages: Any) {
        import from aider.exceptions { LiteLLMExceptions }
        litellm_ex = LiteLLMExceptions();
        if ('deepseek-reasoner' in self.name) {
            messages = ensure_alternating_roles(messages);
        }
        retry_delay = 0.125;
        if self.verbose {
            dump(messages);
        }
        while True {
            try {
                kwargs =
                    {'messages' : messages , 'functions' : None , 'stream' : False };
                (_hash, response) = self.send_completion(_=kwargs);
                res = response.choices[ 0 ].message.content;
                import from aider.reasoning_tags { remove_reasoning_content }
                return remove_reasoning_content(res, self.reasoning_tag);
                except litellm_ex.exceptions_tuple() as err { ex_info =
                    litellm_ex.get_ex_info(err); print(str(err)); if ex_info.description {
                    print(ex_info.description);
                } should_retry = ex_info.retry; if should_retry {
                    retry_delay *= 2;
                if (retry_delay > RETRY_TIMEOUT) {
                    should_retry = False;
                } } if not should_retry {
                    return None;
                } print(f"'Retrying in '{retry_delay}' seconds...'"); time.sleep(retry_delay); continue; }
            if (not response or not hasattr(response, 'choices') or not response.choices ) {
                return None;
            } } except AttributeError {
                return None;
            }
        }
    }
}


def register_models(model_settings_fnames: Any) {
    files_loaded = [];

        for
        model_settings_fname
        in
        model_settings_fnames
        {
        not os.path.exists(model_settings_fname)
            if
            {
            continue;
            }
        not Path(model_settings_fname).read_text().strip()
            if
            {
            continue;
            }
        try {
            with open(model_settings_fname, 'r') as model_settings_file  {
                model_settings_list = yaml.safe_load(model_settings_file);
            }
            for model_settings_dict in model_settings_list {
                model_settings = ModelSettings(_=model_settings_dict);
                MODEL_SETTINGS[ : ] =
                    [ ms for ms in MODEL_SETTINGS if (ms.name != model_settings.name) ];
                MODEL_SETTINGS.append(model_settings);
            }
        } e
            except
            Exception
            as
            {
            raise Exception(
                f"'Error loading model settings from '{model_settings_fname}': '{e}"
            ) ;
            }
        files_loaded.append(model_settings_fname);
        }
    return files_loaded;
}


def register_litellm_models(model_fnames: Any) {
    files_loaded = [];

        for
        model_fname
        in
        model_fnames
        {
        not os.path.exists(model_fname)
            if
            {
            continue;
            }
        try {
            data = Path(model_fname).read_text();
            model_def = json5.loads(data);
            model_info_manager.local_model_metadata.update(model_def);
        not data.strip()
            if
            {
            continue;
            } not model_def
            if
            {
            continue;
            } } e
            except
            Exception
            as
            {
            raise Exception(
                f"'Error loading model definition from '{model_fname}': '{e}"
            ) ;
            }
        files_loaded.append(model_fname);
        }
    return files_loaded;
}


def validate_variables(vars: Any) {
    missing = [];

        for
        var
        in
        vars
        {
        if (var not in os.environ) {
            missing.append(var);
        }
        }
    if missing {
        return <>dict(keys_in_environment=False, missing_keys=missing);
    }
    return <>dict(keys_in_environment=True, missing_keys=missing);
}


def sanity_check_models(io: Any, main_model: Any) {
    problem_main = sanity_check_model(io, main_model);
    problem_weak = None;
    if (main_model.weak_model and (main_model.weak_model is not main_model) ) {
        problem_weak = sanity_check_model(io, main_model.weak_model);
    }
    problem_editor = None;
    if (main_model.editor_model
    and (main_model.editor_model is not main_model)
    and (main_model.editor_model is not main_model.weak_model)
    ) {
        problem_editor = sanity_check_model(io, main_model.editor_model);
    }
    return (problem_main or problem_weak or problem_editor );
}


def sanity_check_model(io: Any, model: Any) {
    show = False;
    if model.missing_keys {
        show = True;
        io.tool_warning(f"'Warning: '{model}' expects these environment variables'");
        for key in model.missing_keys {
            value = os.environ.get(key, '');
            status = 'Set' if value else 'Not set';
            io.tool_output(f"'- '{key}': '{status}");
        }
    if (platform.system() == 'Windows') {
        io.tool_output(
            'Note: You may need to restart your terminal or command prompt for `setx` to take effect.'
        );
    } } elif not model.keys_in_environment {
        show = True;
        io.tool_warning(
            f"'Warning for '{model}': Unknown which environment variables are required.'"
        );
    }
    check_for_dependencies(io, model.name);
    if not model.info {
        show = True;
        io.tool_warning(
            f"'Warning for '{model}': Unknown context window size and costs, using sane defaults.'"
        );
        possible_matches = fuzzy_match_models(model.name);
        if possible_matches {
            io.tool_output('Did you mean one of these?');
            for <>match in possible_matches {
                io.tool_output(f"'- '{<>match}");
            }
        }
    }
    return show;
}


"""\n    Check for model-specific dependencies and install them if needed.\n\n    Args:\n        io: The IO object for user interaction\n        model_name: The name of the model to check dependencies for\n    """
def check_for_dependencies(io: Any, model_name: Any) {
    if model_name.startswith('bedrock/') {
        check_pip_install_extra(
            io,
            'boto3',
            'AWS Bedrock models require the boto3 package.',
            ['boto3']
        );
    } elif model_name.startswith('vertex_ai/') {
        check_pip_install_extra(
            io,
            'google.cloud.aiplatform',
            'Google Vertex AI models require the google-cloud-aiplatform package.',
            ['google-cloud-aiplatform']
        );
    }
}


def fuzzy_match_models(name: Any) {
    name = name.lower();
    chat_models = <>set();
    model_metadata = <>list(litellm.model_cost.items());
    model_metadata += <>list(model_info_manager.local_model_metadata.items());
    for (orig_model, attrs) in model_metadata {
        model = orig_model.lower();
        provider = attrs.get('litellm_provider', '').lower();
        provider += '/';
        if model.startswith(provider) {
            fq_model = orig_model;

                else
                {
                fq_model = (provider + orig_model);
                }

        }
        chat_models.add(fq_model);
        chat_models.add(orig_model);

        if
        (attrs.get('mode') != 'chat')
        {
        continue;
        } not provider
        if
        {
        continue;
        } }
    chat_models = sorted(chat_models);
    matching_models = [ m for m in chat_models if (name in m) ];
    if matching_models {
        return sorted(<>set(matching_models));
    }
    models = <>set(chat_models);
    matching_models = difflib.get_close_matches(name, models, n=3, cutoff=0.8);
    return sorted(<>set(matching_models));
}


def print_matching_models(io: Any, search: Any) {
    matches = fuzzy_match_models(search);
    if matches {
        io.tool_output(f"'Models which match "'{search}'":'");
        for model in matches {
            io.tool_output(f"'- '{model}");
        }
    }
        else
        {
        io.tool_output(f"'No models match "'{search}'".'");
        }

}


def get_model_settings_as_yaml() {
    import from dataclasses { fields }
    import yaml;
    model_settings_list = [];
    defaults = {};
    for field in fields(ModelSettings) {
        defaults[ field.name ] = field.default;
    }
    defaults[ 'name' ] = '(default values)';
    model_settings_list.append(defaults);
    for ms in sorted(MODEL_SETTINGS, key=lambda  x: Any: x.name) {
        model_settings_dict = {};
        for field in fields(ModelSettings) {
            value = getattr(ms, field.name);
        if (value != field.default) {
            model_settings_dict[ field.name ] = value;
        } }
        model_settings_list.append(model_settings_dict);
        model_settings_list.append(None);
    }
    yaml_str =
        yaml.dump(
            [ ms for ms in model_settings_list if (ms is not None) ],
            default_flow_style=False,
            sort_keys=False
        );
    return yaml_str.replace('\n- ', '\n\n- ');
}


def main() {
    if (len(sys.argv) < 2) {
        print('Usage: python models.py <model_name> or python models.py --yaml');
        sys.exit(1);
    }
    if (sys.argv[ 1 ] == '--yaml') {
        yaml_string = get_model_settings_as_yaml();
        print(yaml_string);
    }
        else
        {
        model_name = sys.argv[ 1 ];
        matching_models = fuzzy_match_models(model_name);
        if matching_models {
            print(f""Matching models for '"{model_name}"':"");
            for model in matching_models {
                print(model);
            }
        }
            else
            {
            print(f""No matching models found for '"{model_name}"'."");
            }

        }

}


with entry {
    if (__name__ == '__main__') {
        main();
    }
}

